import csv
import pickle
import numpy as np
import xgboost as xgb
from matplotlib import pyplot
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score, log_loss
from sklearn.model_selection import train_test_split
from sklearn.utils import Bunch
from xgboost import plot_importance, plot_tree
import matplotlib.pyplot as plt
import pandas as pd
from DataSet import X_train, y_train, X_test, y_test, feature_name

# define the datasets to evaluate each iteration
evalset = [(X_train, y_train), (X_test, y_test)]


# define model
model = xgb.XGBClassifier(max_depth=5, n_estimators=50, silent=True, objective='multi:softmax', feature_names=feature_name, num_class=2, booster='gbtree', min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, scale_pos_weight=1)

# train
model.fit(X_train, y_train, eval_metric='mlogloss', verbose=True, eval_set=evalset, early_stopping_rounds=10)

# Draw a loss for training and testing
results = model.evals_result()
# plot learning curves
plt.figure()
plt.plot(results['validation_0']['mlogloss'], label='train')
plt.plot(results['validation_1']['mlogloss'], label='test')
plt.legend()
plt.savefig('picture/mlogloss.png')

# Used to store the trained model
model.save_model('./model/xgb.model')

# Prediction
y_pred = model.predict(X_test)
# predict():No parameters, default output is probability value
print("y_pred:\n", y_pred)
# Calculation accuracy
accuracy = accuracy_score(y_test, y_pred)
print("accuarcy: %.2f%%" % (accuracy*100.0))

# TP FP TN FN
FN=0
for i in range(len(y_test)):
    if y_pred[i]==0 and y_test[i]==1 :
        FN=FN+1
print("FN: %.2f%%" % ((FN/len(y_test))*100.0))

FP=0
for i in range(len(y_test)):
    if y_pred[i]==1 and y_test[i]==0 :
        FP=FP+1
print("FP: %.2f%%" % ((FP/len(y_test))*100.0))

TP=0
for i in range(len(y_test)):
    if y_pred[i]==1 and y_test[i]==1 :
        TP=TP+1
print("TP: %.2f%%" % ((TP/len(y_test))*100.0))

TN=0
for i in range(len(y_test)):
    if y_pred[i]==0 and y_test[i]==0 :
        TN=TN+1
print("TN: %.2f%%" % ((TN/len(y_test))*100.0))

print("Total lightning day:",TP+FN)

# Display important features
plot_importance(model,importance_type ="weight")
plt.savefig('picture/importance_plot.png')


# Visualize the generation of trees, num_trees are the indexes of trees
plot_tree(model, num_trees=5)
plt.savefig('picture/tree.png')

plt.figure()
plt.plot(y_pred[-300:], label='pre')
plt.plot(y_test[-300:], label='true')
plt.legend()
plt.savefig('picture/pre_and_true.png')

# When setting output_margin=True, the output is not the probability value, but the cumulative value of leaf nodes in
# all trees generated by xgboost for each sample
y_pred_output = model.predict(X_test, output_margin=True)
print("y_pred_output:\n", y_pred_output)

# Output the base learner to a txt file
model.get_booster().dump_model("model/model1.txt")

pre_proba = model.predict_proba(X_test)
print("Probability of each sample belonging to each category:\n", pre_proba)

