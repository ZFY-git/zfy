import csv
import pickle
import numpy as np
import xgboost as xgb
from matplotlib import pyplot
from sklearn.datasets import load_iris
from sklearn.metrics import accuracy_score, log_loss
from sklearn.model_selection import train_test_split
from sklearn.utils import Bunch
from xgboost import plot_importance, plot_tree
import matplotlib.pyplot as plt
import pandas as pd

# Loading the model
model = xgb.XGBClassifier()
model.load_model('./model/xgb.model')

csv_name = 'DTXLY_2019.csv'
data = pd.read_csv(f'data/{csv_name}')

# Select the first 4 columns
selected_data = data.iloc[:, :5]
print(selected_data.describe())

# Convert the selected data to a numpy array
selected_data = np.array(selected_data)

# # Reshape the selected data to a dictionary
# selected_data = {'data': selected_data}

selected_target = data.iloc[:,5]

# Convert the selected data to a numpy array
selected_target = np.array(selected_target)
#
# # Reshape the selected data to a dictionary
# selected_target = {'target': selected_target}

X,y = selected_data,selected_target

# Prediction
y_pred = model.predict(X)
# predict():No parameters, default output is probability value
print("y_pred:\n", y_pred)
print("y_test:\n", y)
# Calculation accuracy
accuracy = accuracy_score(y, y_pred)
print("accuarcy: %.2f%%" % (accuracy*100.0))

# TP FP TN FN
FN=0
for i in range(len(y)):
    if y_pred[i]==0 and y[i]==1 :
        FN=FN+1
print("FN: %.2f%%" % ((FN/len(y))*100.0))

FP=0
for i in range(len(y)):
    if y_pred[i]==1 and y[i]==0 :
        FP=FP+1
print("FP: %.2f%%" % ((FP/len(y))*100.0))

TP=0
for i in range(len(y)):
    if y_pred[i]==1 and y[i]==1 :
        TP=TP+1
print("TP: %.2f%%" % ((TP/len(y))*100.0))

TN=0
for i in range(len(y)):
    if y_pred[i]==0 and y[i]==0 :
        TN=TN+1
print("TN: %.2f%%" % ((TN/len(y))*100.0))

print("Total lightning day:",TP+FN)
print("TP:", TP)
print("FN:", FN)

# Display important features
plot_importance(model,importance_type ="weight")
plt.savefig('picture/predict/importance.png')
plt.clf()
# Visualize the generation of trees, num_trees are the indexes of trees
#plot_tree(model, num_trees=2)
#plt.savefig('picture/tree.png')
#plt.clf()


plt.plot(y_pred[-300:], label='pre')
plt.plot(y[-300:], label='true')
plt.legend()
plt.savefig('picture/predict/pre_and_true.png')


# When setting output_margin=True, the output is not the probability value, but the cumulative value of leaf nodes in
# all trees generated by xgboost for each sample
y_pred_output = model.predict(X, output_margin=True)
print("y_pred_output:\n", y_pred_output)

# Output the base learner to a txt file
model.get_booster().dump_model("model/model1.txt")

pre_proba = model.predict_proba(X)
print("Probability of each sample belonging to each category:\n", pre_proba)
